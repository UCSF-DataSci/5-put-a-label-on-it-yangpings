{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T21:58:37.738446Z",
     "start_time": "2025-05-07T21:58:36.847759Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T21:59:40.784328Z",
     "start_time": "2025-05-07T21:59:40.781066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the synthetic health data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the data\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Load the CSV file using pandas\n",
    "    path = file_path\n",
    "    df = pd.read_csv(\n",
    "            file_path,\n",
    "            parse_dates=[\"timestamp\"],   # parses the column to datetime dtype\n",
    "            infer_datetime_format=True)\n",
    "    return df"
   ],
   "id": "90fe19288f6aec59",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:02:02.822272Z",
     "start_time": "2025-05-07T22:02:02.817271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data_part1(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling: select features, split into train/test sets, handle missing values.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        test_size: Proportion of data for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Select relevant features (age, systolic_bp, diastolic_bp, glucose_level, bmi)\n",
    "    # 2. Select target variable (disease_outcome)\n",
    "    # 3. Split data into training and testing sets\n",
    "    # 4. Handle missing values using SimpleImputer\n",
    "    \n",
    "    # Placeholder return - replace with your implementation\n",
    "    feature_cols = [\"age\", \"systolic_bp\", \"diastolic_bp\",\n",
    "                    \"glucose_level\", \"bmi\"]\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"disease_outcome\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y         \n",
    "    )\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=feature_cols,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=feature_cols,\n",
    "        index=X_test.index\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "d78c3f6c99a23db5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:05:13.355667Z",
     "start_time": "2025-05-07T22:05:13.351661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        \n",
    "    Returns:\n",
    "        Trained logistic regression model\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Initialize and train a LogisticRegression model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ],
   "id": "23f6807e60b6c6e6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:06:10.499229Z",
     "start_time": "2025-05-07T22:06:10.491229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_evaluation_metrics(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate classification evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing accuracy, precision, recall, f1, auc, and confusion_matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    " # 1. Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 2. Metrics\n",
    "    accuracy  = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0, average=\"binary\" if len(np.unique(y_test)) == 2 else \"weighted\")\n",
    "    recall    = recall_score(y_test, y_pred, zero_division=0, average=\"binary\" if len(np.unique(y_test)) == 2 else \"weighted\")\n",
    "    f1        = f1_score(y_test, y_pred, zero_division=0, average=\"binary\" if len(np.unique(y_test)) == 2 else \"weighted\")\n",
    "\n",
    "    # AUC: only meaningful if probabilities available and at least two classes\n",
    "    auc = None\n",
    "    if hasattr(model, \"predict_proba\") and len(np.unique(y_test)) >= 2:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        if y_proba.shape[1] == 2:  # binary\n",
    "            auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "        else:                      # multiclass\n",
    "            auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\", average=\"weighted\")\n",
    "\n",
    "    # 3. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # 4. Return results\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }"
   ],
   "id": "ab75f3215a113526",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:14:19.957726Z",
     "start_time": "2025-05-07T22:14:19.953831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create results directory and save metrics\n",
    "# YOUR CODE HERE\n",
    "def save_metrics_to_file(metrics,\n",
    "                         results_dir: str = \"results\",\n",
    "                         file_name: str = \"results_part1.txt\",\n",
    "                         float_fmt: str = \"{:.4f}\"):\n",
    "    # 1. Ensure the directory exists\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    file_path = os.path.join(results_dir, file_name)\n",
    "\n",
    "    # 2. Helper to convert each value to a readable string\n",
    "    def _to_string(val):\n",
    "        if isinstance(val, float):\n",
    "            return float_fmt.format(val)\n",
    "        if isinstance(val, np.ndarray):\n",
    "            return \"\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in val)\n",
    "        return str(val)\n",
    "\n",
    "    # 3. Write out the metrics\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f\"{k}: {_to_string(v)}\\n\")\n",
    "\n",
    "    return file_path"
   ],
   "id": "9db56432fb6bdda9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:13:25.970626Z",
     "start_time": "2025-05-07T22:13:25.960279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def interpret_results(metrics):\n",
    "    \"\"\"\n",
    "    Analyze model performance on imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "        - 'best_metric': Name of the metric that performed best\n",
    "        - 'worst_metric': Name of the metric that performed worst\n",
    "        - 'imbalance_impact_score': A score from 0-1 indicating how much\n",
    "          the class imbalance affected results (0=no impact, 1=severe impact)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    scalar_keys = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "    scalar_metrics = {k: v for k, v in metrics.items()\n",
    "                      if k in scalar_keys and v is not None}\n",
    "\n",
    "    if not scalar_metrics:\n",
    "        raise ValueError(\"No scalar metrics found in the input dictionary.\")\n",
    "\n",
    "    best_metric  = max(scalar_metrics, key=scalar_metrics.get)\n",
    "    worst_metric = min(scalar_metrics, key=scalar_metrics.get)\n",
    "\n",
    "    acc    = scalar_metrics.get(\"accuracy\", 0.0)\n",
    "    f1     = scalar_metrics.get(\"f1\", acc)        # fallback to acc if missing\n",
    "    recall = scalar_metrics.get(\"recall\", acc)\n",
    "\n",
    "    imbalance_impact_score = (abs(acc - f1) + abs(acc - recall)) / 2.0\n",
    "\n",
    "    return {\n",
    "        \"best_metric\": best_metric,\n",
    "        \"worst_metric\": worst_metric,\n",
    "        \"imbalance_impact_score\": float(round(imbalance_impact_score, 4))\n",
    "    }"
   ],
   "id": "c8f62466b6f6c9fd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:35:23.119401Z",
     "start_time": "2025-05-07T22:35:23.062075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load data\n",
    "    data_file = 'data//synthetic_health_data.csv'\n",
    "    df = load_data(data_file)\n",
    "\n",
    "    # 2. Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data_part1(df)\n",
    "\n",
    "    # 3. Train model\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "\n",
    "    # 4. Evaluate model\n",
    "    metrics = calculate_evaluation_metrics(model, X_test, y_test)\n",
    "\n",
    "    # 5. Print metrics\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'confusion_matrix':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # 6. Save results\n",
    "    save_metrics_to_file(metrics)\n",
    "\n",
    "    # 7. Interpret results\n",
    "    interpretation = interpret_results(metrics)\n",
    "    print(\"\\nResults Interpretation:\")\n",
    "    for key, value in interpretation.items():\n",
    "        print(f\"{key}: {value}\")"
   ],
   "id": "cc01fc05f3173d4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9195\n",
      "precision: 0.6765\n",
      "recall: 0.3239\n",
      "f1: 0.4381\n",
      "auc: 0.8852\n",
      "\n",
      "Results Interpretation:\n",
      "best_metric: accuracy\n",
      "worst_metric: recall\n",
      "imbalance_impact_score: 0.5385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericy\\AppData\\Local\\Temp\\ipykernel_35600\\3215795810.py:14: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
